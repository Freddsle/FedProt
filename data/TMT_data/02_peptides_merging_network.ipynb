{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center1 done\n",
      "Number of peptides: 5958\n",
      "Center2 done\n",
      "Number of peptides: 6303\n",
      "Center3 done\n",
      "Number of peptides: 5528\n"
     ]
    }
   ],
   "source": [
    "# # CLIENT LEVEL\n",
    "# path_to_data = '/home/yuliya/repos/cosybio/FedProt/data/TMT_data/balanced_data/04_Peptides_PG'\n",
    "# feature_column = 'Proteins'\n",
    "\n",
    "path_to_data = '/home/yuliya/repos/cosybio/FedProt/data/TMT_data/balanced_data/03_Peptides_Genes'\n",
    "feature_column = 'Gene.names'\n",
    "\n",
    "centers = ['Center1', 'Center2', 'Center3']\n",
    "\n",
    "intensities = {}\n",
    "mappings = {}\n",
    "\n",
    "for center in centers:\n",
    "    aggregated_report = pd.read_csv(f'{path_to_data}/{center}/aggregated_NF.tsv', sep='\\t')\n",
    "    # add all to intensities dict with key = center\n",
    "    intensities[center] = aggregated_report\n",
    "    # add all to mappings dict with key = center\n",
    "    mapping = aggregated_report[['Sequence', feature_column]]\n",
    "    mappings[center] = mapping\n",
    "    print(f'{center} done')\n",
    "    print(f'Number of peptides: {len(aggregated_report)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of peptides in merged mapping: 7854\n",
      "Number of peptides in merged mapping without NAs: 7628\n"
     ]
    }
   ],
   "source": [
    "# SERCER side --- merge mappings from different centers\n",
    "\n",
    "merged_mapping = None\n",
    "\n",
    "for name, df in mappings.items():\n",
    "    if merged_mapping is None:\n",
    "        merged_mapping = df\n",
    "    else:\n",
    "        merged_mapping = pd.merge(merged_mapping, df, on=['Sequence', feature_column], how='outer')\n",
    "\n",
    "print(f'Number of peptides in merged mapping: {len(merged_mapping)}')\n",
    "# remove rows with NA\n",
    "merged_mapping = merged_mapping.dropna(subset=['Sequence', feature_column])\n",
    "print(f'Number of peptides in merged mapping without NAs: {len(merged_mapping)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique peptides: 7567\n"
     ]
    }
   ],
   "source": [
    "def find_union(group, feature_column='Proteins'):\n",
    "    # Find the intersection of lists in \"Gene.names\" within the group\n",
    "    union = set(group[feature_column].iloc[0])\n",
    "    for names in group[feature_column]:\n",
    "        union |= set(names)\n",
    "    # Return a Series with \"Sequences\" and the intersection of \"Gene.names\"\n",
    "    return pd.Series({\n",
    "        feature_column: ';'.join(union)\n",
    "        # 'Gene.names': intersection\n",
    "    })\n",
    "\n",
    "\n",
    "merged_mapping[feature_column] = merged_mapping[feature_column].str.split(';')\n",
    "merged_mapping = merged_mapping.groupby('Sequence').apply(lambda x: find_union(x, feature_column)).reset_index()\n",
    "\n",
    "print(f'Number of unique peptides: {len(merged_mapping)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### server side --- calculate unique and razor\n",
    "\n",
    "df_exploded = merged_mapping.assign(**{feature_column: merged_mapping[feature_column].str.split(';')}).explode(feature_column)\n",
    "\n",
    "unique_razor = df_exploded[feature_column].value_counts().rename_axis(feature_column).reset_index(name='Unique_razor')\n",
    "unique_counts = merged_mapping[merged_mapping[feature_column].str.contains(';') == False][feature_column].value_counts().rename_axis(feature_column).reset_index(name='Unique')\n",
    "\n",
    "result = pd.merge(unique_razor, unique_counts, on=feature_column, how='left').fillna({'Unique': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptides_mappings = pd.Series(merged_mapping[feature_column].values, index=merged_mapping['Sequence']).to_dict()\n",
    "peptides_mapping_dict = {key: set(value.split(';')) for key, value in peptides_mappings.items()}\n",
    "\n",
    "razor_uniq_dict = pd.Series(result['Unique_razor'].values, index=result[feature_column]).to_dict()\n",
    "unique_genes = result[result['Unique'] > 0][feature_column].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_leading(features, more_then_half=False):\n",
    "    # Filter the counts for the genes in the set\n",
    "    relevant_counts = {feature: razor_uniq_dict[feature] for feature in features}\n",
    "    # Find the max count\n",
    "    max_count = max(relevant_counts.values())\n",
    "    \n",
    "    if more_then_half:\n",
    "        leading_genes = [feature for feature, count in relevant_counts.items() if count >= max_count / 2]\n",
    "    \n",
    "    else:\n",
    "        leading_genes = [feature for feature, count in relevant_counts.items() if count  == max_count]\n",
    "\n",
    "    return set(sorted(leading_genes))\n",
    "\n",
    "\n",
    "\n",
    "def build_peptide_gene_graph(peptides_mapping_dict):\n",
    "    \"\"\"\n",
    "    Build a graph where peptides and proteins are nodes.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    for peptide, features in peptides_mapping_dict.items():\n",
    "        for feature in features:\n",
    "            G.add_node(peptide, type='peptide')\n",
    "            G.add_node(feature, type='feature')\n",
    "            G.add_edge(peptide, feature)\n",
    "    return G\n",
    "\n",
    "\n",
    "def sort_by_count(features_list):\n",
    "    if len(features_list) == 1:\n",
    "        return features_list\n",
    "    # sort by razor unique count, from biggest to smallest\n",
    "    return sorted(list(set(features_list)), key=lambda x: razor_uniq_dict[x], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = build_peptide_gene_graph(peptides_mapping_dict)\n",
    "connected_components = nx.connected_components(G)\n",
    "\n",
    "final_protein_groups = []\n",
    "\n",
    "for component in connected_components:\n",
    "    component_copy = set(component)  \n",
    "    features = [node for node in component_copy if G.nodes[node]['type'] == 'feature']\n",
    "    peptides = [node for node in component_copy if G.nodes[node]['type'] == 'peptide']\n",
    "    \n",
    "    # find proteins with max count\n",
    "    for feature in features:\n",
    "        \n",
    "        if len(peptides) == 0 or len(features) == 0:\n",
    "            print('Error')\n",
    "            print(f'Peptides: {peptides}')\n",
    "            print(f'Features: {features}')\n",
    "            raise ValueError('Peptides or features are not empty')\n",
    "       \n",
    "        leading = list(find_leading(features))\n",
    "        leading_unique = set(leading) & set(unique_genes)\n",
    "        if len(leading) > 1 and  len(leading_unique) > 0:\n",
    "            # if intersect with unique_genes - take the first unique gene\n",
    "            leading = list(leading_unique)[0]\n",
    "        else:\n",
    "            leading = leading[0]\n",
    "                \n",
    "        \n",
    "        # Get all peptides directly connected to the leading protein\n",
    "        leading_peptides = [peptide for peptide in peptides if G.has_edge(leading, peptide)]\n",
    "        # Find other proteins connected only to these leading peptides\n",
    "        other = set()\n",
    "\n",
    "        for peptide in leading_peptides:\n",
    "            connected_proteins = set(G.neighbors(peptide)) & set(features)  # Proteins connected to this peptide\n",
    "            \n",
    "            # Filter out proteins that are connected to peptides not in leading_peptides\n",
    "            valid_features = set()\n",
    "            for protein in connected_proteins:\n",
    "                protein_peptides = set(G.neighbors(protein)) & set(peptides)  # Peptides connected to this protein\n",
    "                if protein_peptides.issubset(set(leading_peptides)):\n",
    "                    valid_features.add(protein)\n",
    "\n",
    "            # Update 'other' with proteins that meet the criteria\n",
    "            other.update(valid_features)\n",
    "\n",
    "        other.discard(leading)\n",
    "        leading_group = list(other) + [leading]\n",
    "\n",
    "        razor_feature = find_leading(leading_group, more_then_half=False)\n",
    "        if len(razor_feature) > 1:\n",
    "            # keep only the first unique if there are more than one and unique is present\n",
    "            if len(razor_feature & set(unique_genes)) > 0:\n",
    "                razor_feature = sort_by_count(list(razor_feature & set(unique_genes)))\n",
    "        razor_feature = list(razor_feature)[0]\n",
    "        \n",
    "        final_protein_group = {\n",
    "            'features': sort_by_count(leading_group),\n",
    "            'peptides': sorted(list(set(leading_peptides))),\n",
    "            'razor_feature': razor_feature,\n",
    "            'major_features': sort_by_count(list(find_leading(leading_group, more_then_half=True))),\n",
    "        }\n",
    "        \n",
    "        final_protein_groups.append(final_protein_group)\n",
    "            \n",
    "        # Remove the proteins and peptides from the component_copy\n",
    "        to_remove = set(leading_group) | set(leading_peptides)\n",
    "        component_copy -= to_remove\n",
    "        features = [feature for feature in features if feature not in set(leading_group)]\n",
    "        peptides = [peptide for peptide in peptides if peptide not in set(leading_peptides)]\n",
    "\n",
    "        if len(peptides) == 0 and len(features) == 0:\n",
    "            break\n",
    "    \n",
    "    if len(peptides) == 0 and len(features) == 0:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(612, 4)\n",
      "(7567, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>peptides</th>\n",
       "      <th>razor_feature</th>\n",
       "      <th>major_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SERPINA3</td>\n",
       "      <td>AAAATGTIFTFR</td>\n",
       "      <td>SERPINA3</td>\n",
       "      <td>SERPINA3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SERPINA3</td>\n",
       "      <td>ADLSGITGAR</td>\n",
       "      <td>SERPINA3</td>\n",
       "      <td>SERPINA3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SERPINA3</td>\n",
       "      <td>AKWEMPFDPQDTHQSR</td>\n",
       "      <td>SERPINA3</td>\n",
       "      <td>SERPINA3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SERPINA3</td>\n",
       "      <td>AVLDVFEEGTEASAATAVK</td>\n",
       "      <td>SERPINA3</td>\n",
       "      <td>SERPINA3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SERPINA3</td>\n",
       "      <td>AVVEVDESGTR</td>\n",
       "      <td>SERPINA3</td>\n",
       "      <td>SERPINA3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features             peptides razor_feature major_features\n",
       "0  SERPINA3         AAAATGTIFTFR      SERPINA3       SERPINA3\n",
       "0  SERPINA3           ADLSGITGAR      SERPINA3       SERPINA3\n",
       "0  SERPINA3     AKWEMPFDPQDTHQSR      SERPINA3       SERPINA3\n",
       "0  SERPINA3  AVLDVFEEGTEASAATAVK      SERPINA3       SERPINA3\n",
       "0  SERPINA3          AVVEVDESGTR      SERPINA3       SERPINA3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_protein_groups_df = pd.DataFrame(final_protein_groups)\n",
    "final_protein_groups_df['major_features'] = final_protein_groups_df['major_features'].apply(lambda x: ';'.join(x))\n",
    "final_protein_groups_df['features'] = final_protein_groups_df['features'].apply(lambda x: ';'.join(x))\n",
    "print(final_protein_groups_df.shape)\n",
    "\n",
    "# split peptides to different rows \n",
    "final_matching = final_protein_groups_df.reset_index(drop=True).explode('peptides')\n",
    "\n",
    "print(final_matching.shape)\n",
    "final_matching.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for center in centers:\n",
    "    # write to file\n",
    "    final_matching.to_csv(f'{path_to_data}/{center}/mapping.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggragate peptides to PG / Genes using mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for center in centers:\n",
    "    aggregated_report = pd.read_csv(f'{path_to_data}/{center}/aggregated_NF.tsv', sep='\\t')\n",
    "    final_matching = pd.read_csv(f'{path_to_data}/{center}/mapping.tsv', sep='\\t')\n",
    "\n",
    "    ################################  MERGE MAPPING WITH AGGREGATED REPORT  ################################\n",
    "    # rename peptides to Sequence\n",
    "    final_matching = final_matching.rename(columns={'peptides': 'Sequence'})\n",
    "    # merge final_matching with aggregated_report (left join) using 'Sequence' as a key\n",
    "    aggregated_report = pd.merge(aggregated_report, final_matching, on='Sequence', how='left')\n",
    "    # write to file\n",
    "    aggregated_report.to_csv(f'{path_to_data}/{center}/aggregated_NF_mapped.tsv', sep='\\t', index=False)\n",
    "    # remove unnecessary columns\n",
    "    intensities[center] = aggregated_report.drop(columns=['Sequence', 'Proteins', 'Gene.names', 'features', 'razor_feature'])\n",
    "    # drop column where major_features is NA\n",
    "    intensities[center].dropna(subset=['major_features'], inplace=True)\n",
    "\n",
    "    ################################  CHECK FOR CONTAMINANTS  ################################\n",
    "    # Check for contaminants\n",
    "    #  group by major_features and sum intensities for each group (if there is NA in any column, it will be ignored)\n",
    "    reverse_contaminants = intensities[center].groupby('major_features').agg({**{col: 'sum' for col in intensities[center].columns if 'intensity' in col},\n",
    "                                            'Reverse': lambda x: (x == '+').sum(),\n",
    "                                            'Potential.contaminant': lambda x: (x == '+').sum(),\n",
    "                                            'major_features': 'size'})\n",
    "\n",
    "    reverse_contaminants['Reverse_mark'] = reverse_contaminants.apply(lambda x: '+' if x['Reverse'] > x['major_features'] / 2 else 'NA', axis=1)\n",
    "    reverse_contaminants['Potential.contaminant_mark'] = reverse_contaminants.apply(lambda x: '+' if x['Potential.contaminant'] > x['major_features'] / 2 else 'NA', axis=1)\n",
    "    # rename major_features to unique  + razor peptide\n",
    "    reverse_contaminants = reverse_contaminants.rename(columns={'major_features': 'unique_razor_counts'})\n",
    "    reverse_contaminants.drop(columns=['Reverse', 'Potential.contaminant'], inplace=True)\n",
    "\n",
    "    ################################  CALCULATE INTENSITIES  ################################\n",
    "    # Group by major_features and sum intensities for each group\n",
    "    intensities[center].drop(columns=['Reverse', 'Potential.contaminant'], inplace=True)\n",
    "    intensities[center] = intensities[center].groupby('major_features').sum().reset_index()\n",
    "    intensities[center] = pd.merge(intensities[center], reverse_contaminants, on='major_features', how='left')\n",
    "\n",
    "    ################################  WRITE TO FILE  ################################\n",
    "    # write to file\n",
    "    intensities[center].to_csv(f'{path_to_data}/{center}/intensities_counts_ALL.tsv', sep='\\t', index=False)\n",
    "\n",
    "    # filter out contaminants and reverse hits\n",
    "    intensities[center] = intensities[center][intensities[center]['Reverse_mark'] != '+']\n",
    "    intensities[center] = intensities[center][intensities[center]['Potential.contaminant_mark'] != '+']\n",
    "    intensities[center].drop(columns=['Reverse_mark', 'Potential.contaminant_mark'], inplace=True)\n",
    "\n",
    "    # save only intensities to file (without unique_razor_counts column)\n",
    "    intensities[center].drop(columns=['unique_razor_counts']).to_csv(f'{path_to_data}/{center}/intensities_filtered.tsv', sep='\\t', index=False)\n",
    "    # save only unique_razor_counts to file\n",
    "    intensities[center][['major_features', 'unique_razor_counts']].to_csv(f'{path_to_data}/{center}/counts_filtered.tsv', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on Center 2 values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center2 done\n",
      "Number of peptides: 6303\n",
      "Number of peptides in merged mapping without NAs: 6143\n"
     ]
    }
   ],
   "source": [
    "# # CLIENT LEVEL\n",
    "# path_to_data = '/home/yuliya/repos/cosybio/FedProt/data/TMT_data/balanced_data/04_Peptides_PG'\n",
    "# output = '/home/yuliya/repos/cosybio/FedProt/data/TMT_data/balanced_data/Check_center_2'\n",
    "# feature_column = 'Proteins'\n",
    "\n",
    "path_to_data = '/home/yuliya/repos/cosybio/FedProt/data/TMT_data/balanced_data/03_Peptides_Genes'\n",
    "output = '/home/yuliya/repos/cosybio/FedProt/data/TMT_data/balanced_data/Check_center_2'\n",
    "feature_column = 'Gene.names'\n",
    "\n",
    "center = 'Center2'\n",
    "\n",
    "aggregated_report = pd.read_csv(f'{path_to_data}/{center}/aggregated_NF.tsv', sep='\\t')\n",
    "# add all to intensities dict with key = center\n",
    "aggregated_report = aggregated_report\n",
    "# add all to mappings dict with key = center\n",
    "mapping = aggregated_report[['Sequence', feature_column]]\n",
    "merged_mapping = mapping\n",
    "print(f'{center} done')\n",
    "print(f'Number of peptides: {len(aggregated_report)}')\n",
    "# remove rows with NA\n",
    "merged_mapping = merged_mapping.dropna(subset=['Sequence', feature_column])\n",
    "print(f'Number of peptides in merged mapping without NAs: {len(merged_mapping)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "### server side --- calculate unique and razor\n",
    "\n",
    "df_exploded = merged_mapping.assign(**{feature_column: merged_mapping[feature_column].str.split(';')}).explode(feature_column)\n",
    "\n",
    "unique_razor = df_exploded[feature_column].value_counts().rename_axis(feature_column).reset_index(name='Unique_razor')\n",
    "unique_counts = merged_mapping[merged_mapping[feature_column].str.contains(';') == False][feature_column].value_counts().rename_axis(feature_column).reset_index(name='Unique')\n",
    "\n",
    "result = pd.merge(unique_razor, unique_counts, on=feature_column, how='left').fillna({'Unique': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptides_mappings = pd.Series(merged_mapping[feature_column].values, index=merged_mapping['Sequence']).to_dict()\n",
    "peptides_mapping_dict = {key: set(value.split(';')) for key, value in peptides_mappings.items()}\n",
    "\n",
    "razor_uniq_dict = pd.Series(result['Unique_razor'].values, index=result[feature_column]).to_dict()\n",
    "unique_genes = result[result['Unique'] > 0][feature_column].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_leading(features, more_then_half=False):\n",
    "    # Filter the counts for the genes in the set\n",
    "    relevant_counts = {feature: razor_uniq_dict[feature] for feature in features}\n",
    "    # Find the max count\n",
    "    max_count = max(relevant_counts.values())\n",
    "    \n",
    "    if more_then_half:\n",
    "        leading_genes = [feature for feature, count in relevant_counts.items() if count >= max_count / 2]\n",
    "    \n",
    "    else:\n",
    "        leading_genes = [feature for feature, count in relevant_counts.items() if count == max_count]\n",
    "\n",
    "    return set(sorted(leading_genes))\n",
    "\n",
    "\n",
    "\n",
    "def build_peptide_gene_graph(peptides_mapping_dict):\n",
    "    \"\"\"\n",
    "    Build a graph where peptides and proteins are nodes.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    for peptide, features in peptides_mapping_dict.items():\n",
    "        for feature in features:\n",
    "            G.add_node(peptide, type='peptide')\n",
    "            G.add_node(feature, type='feature')\n",
    "            G.add_edge(peptide, feature)\n",
    "    return G\n",
    "\n",
    "\n",
    "def sort_by_count(features_list):\n",
    "    if len(features_list) == 1:\n",
    "        return features_list\n",
    "    # sort by razor unique count, from biggest to smallest\n",
    "    return sorted(list(set(features_list)), key=lambda x: razor_uniq_dict[x], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = build_peptide_gene_graph(peptides_mapping_dict)\n",
    "connected_components = nx.connected_components(G)\n",
    "\n",
    "final_protein_groups = []\n",
    "\n",
    "for component in connected_components:\n",
    "    component_copy = set(component)  \n",
    "    features = [node for node in component_copy if G.nodes[node]['type'] == 'feature']\n",
    "    peptides = [node for node in component_copy if G.nodes[node]['type'] == 'peptide']\n",
    "    \n",
    "    # find proteins with max count\n",
    "    for feature in features:\n",
    "        \n",
    "        if len(peptides) == 0 or len(features) == 0:\n",
    "            print('Error')\n",
    "            print(f'Peptides: {peptides}')\n",
    "            print(f'Features: {features}')\n",
    "            raise ValueError('Peptides or features are not empty')\n",
    "       \n",
    "        leading = list(find_leading(features))\n",
    "        leading_unique = set(leading) & set(unique_genes)\n",
    "        if len(leading) > 1 and  len(leading_unique) > 0:\n",
    "            # if intersect with unique_genes - take the first unique gene\n",
    "            leading = list(leading_unique)[0]\n",
    "        else:\n",
    "            leading = leading[0]\n",
    "                \n",
    "        \n",
    "        # Get all peptides directly connected to the leading protein\n",
    "        leading_peptides = [peptide for peptide in peptides if G.has_edge(leading, peptide)]\n",
    "        # Find other proteins connected only to these leading peptides\n",
    "        other = set()\n",
    "\n",
    "        for peptide in leading_peptides:\n",
    "            connected_proteins = set(G.neighbors(peptide)) & set(features)  # Proteins connected to this peptide\n",
    "            \n",
    "            # Filter out proteins that are connected to peptides not in leading_peptides\n",
    "            valid_features = set()\n",
    "            for protein in connected_proteins:\n",
    "                protein_peptides = set(G.neighbors(protein)) & set(peptides)  # Peptides connected to this protein\n",
    "                if protein_peptides.issubset(set(leading_peptides)):\n",
    "                    valid_features.add(protein)\n",
    "\n",
    "            # Update 'other' with proteins that meet the criteria\n",
    "            other.update(valid_features)\n",
    "\n",
    "        other.discard(leading)\n",
    "        leading_group = list(other) + [leading]\n",
    "\n",
    "        razor_feature = find_leading(leading_group, more_then_half=False)\n",
    "        if len(razor_feature) > 1:\n",
    "            # keep only the first unique if there are more than one and unique is present\n",
    "            if len(razor_feature & set(unique_genes)) > 0:\n",
    "                razor_feature = sort_by_count(list(razor_feature & set(unique_genes)))\n",
    "        razor_feature = list(razor_feature)[0]\n",
    "        \n",
    "        final_protein_group = {\n",
    "            'features': sort_by_count(leading_group),\n",
    "            'peptides': sorted(list(set(leading_peptides))),\n",
    "            'razor_feature': razor_feature,\n",
    "            'major_features': sort_by_count(list(find_leading(leading_group, more_then_half=True))),\n",
    "        }\n",
    "        \n",
    "        final_protein_groups.append(final_protein_group)\n",
    "            \n",
    "        # Remove the proteins and peptides from the component_copy\n",
    "        to_remove = set(leading_group) | set(leading_peptides)\n",
    "        component_copy -= to_remove\n",
    "        features = [feature for feature in features if feature not in set(leading_group)]\n",
    "        peptides = [peptide for peptide in peptides if peptide not in set(leading_peptides)]\n",
    "\n",
    "        if len(peptides) == 0 and len(features) == 0:\n",
    "            break\n",
    "    \n",
    "    if len(peptides) == 0 and len(features) == 0:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(526, 4)\n",
      "(6143, 4)\n",
      "Number of proteins in grouped data: 526\n"
     ]
    }
   ],
   "source": [
    "final_protein_groups_df = pd.DataFrame(final_protein_groups)\n",
    "final_protein_groups_df['major_features'] = final_protein_groups_df['major_features'].apply(lambda x: ';'.join(x))\n",
    "final_protein_groups_df['features'] = final_protein_groups_df['features'].apply(lambda x: ';'.join(x))\n",
    "print(final_protein_groups_df.shape)\n",
    "# split peptides to different rows \n",
    "final_matching = final_protein_groups_df.reset_index(drop=True).explode('peptides')\n",
    "print(final_matching.shape)\n",
    "final_matching.head()\n",
    "print(f\"Number of proteins in grouped data: {len(set(final_matching['major_features'].drop_duplicates().values))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_matching.to_csv(f'{output}/mapping.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique peptides: 501\n"
     ]
    }
   ],
   "source": [
    "aggregated_report = pd.read_csv(f'{path_to_data}/{center}/aggregated_NF.tsv', sep='\\t')\n",
    "final_matching = pd.read_csv(f'{output}/mapping.tsv', sep='\\t')\n",
    "\n",
    "################################  MERGE MAPPING WITH AGGREGATED REPORT  ################################\n",
    "# rename peptides to Sequence\n",
    "final_matching = final_matching.rename(columns={'peptides': 'Sequence'})\n",
    "# merge final_matching with aggregated_report (left join) using 'Sequence' as a key\n",
    "aggregated_report = pd.merge(aggregated_report, final_matching, on='Sequence', how='left')\n",
    "# write to file\n",
    "aggregated_report.to_csv(f'{output}/aggregated_NF_mapped{feature_column}.tsv', sep='\\t', index=False)\n",
    "# remove unnecessary columns\n",
    "aggregated_report = aggregated_report.drop(columns=['Sequence', 'Proteins', 'Gene.names', 'features', 'razor_feature'])\n",
    "# drop column where major_features is NA\n",
    "aggregated_report.dropna(subset=['major_features'], inplace=True)\n",
    "\n",
    "################################  CHECK FOR CONTAMINANTS  ################################\n",
    "# Check for contaminants\n",
    "#  group by major_features and sum intensities for each group (if there is NA in any column, it will be ignored)\n",
    "reverse_contaminants = aggregated_report.groupby('major_features').agg({**{col: 'sum' for col in aggregated_report.columns if 'intensity' in col},\n",
    "                                        'Reverse': lambda x: (x == '+').sum(),\n",
    "                                        'Potential.contaminant': lambda x: (x == '+').sum(),\n",
    "                                        'major_features': 'size'})\n",
    "\n",
    "reverse_contaminants['Reverse_mark'] = reverse_contaminants.apply(lambda x: '+' if x['Reverse'] >= x['major_features'] / 2 else 'NA', axis=1)\n",
    "reverse_contaminants['Potential.contaminant_mark'] = reverse_contaminants.apply(lambda x: '+' if x['Potential.contaminant'] >= x['major_features'] / 2 else 'NA', axis=1)\n",
    "# rename major_features to unique  + razor peptide\n",
    "reverse_contaminants = reverse_contaminants.rename(columns={'major_features': 'unique_razor_counts'})\n",
    "reverse_contaminants.drop(columns=['Reverse', 'Potential.contaminant'], inplace=True)\n",
    "\n",
    "################################  CALCULATE INTENSITIES  ################################\n",
    "# Group by major_features and sum intensities for each group\n",
    "aggregated_report.drop(columns=['Reverse', 'Potential.contaminant'], inplace=True)\n",
    "aggregated_report = aggregated_report.groupby('major_features').sum().reset_index()\n",
    "aggregated_report = pd.merge(aggregated_report, reverse_contaminants, on='major_features', how='left')\n",
    "\n",
    "################################  WRITE TO FILE  ################################\n",
    "# write to file\n",
    "aggregated_report.to_csv(f'{output}/intensities_counts_ALL_pep_to{feature_column}.tsv', sep='\\t', index=False)\n",
    "\n",
    "# filter out contaminants and reverse hits\n",
    "aggregated_report = aggregated_report[aggregated_report['Reverse_mark'] != '+']\n",
    "aggregated_report = aggregated_report[aggregated_report['Potential.contaminant_mark'] != '+']\n",
    "aggregated_report.drop(columns=['Reverse_mark', 'Potential.contaminant_mark'], inplace=True)\n",
    "\n",
    "# save only intensities to file (without unique_razor_counts column)\n",
    "aggregated_report.drop(columns=['unique_razor_counts']).to_csv(f'{output}/intensities_filtered_pep_to{feature_column}.tsv', sep='\\t', index=False)\n",
    "print(f'Number of unique peptides: {len(aggregated_report)}')\n",
    "# save only unique_razor_counts to file\n",
    "aggregated_report[['major_features', 'unique_razor_counts']].to_csv(f'{output}/counts_filtered_pep_to{feature_column}.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedprot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
